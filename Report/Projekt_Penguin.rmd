---
title: "Project Penguin"
author: "Florian Mayer, Clemens Pichler"
date: "2024-01-23"
output: pdf_document
toc: true
toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(rstan) 
library(bayesplot)
library(knitr)

penguins <- readRDS("../Data/penguins.RDS")

penguins$sex_f <- as.integer(factor(penguins$sex))
penguins$species_f <- as.integer(factor(penguins$species))
```



# 1. Data exploration
In our data exploration of the "penguins" dataset, which consists of 333 observations across four variables: the two categorical variables 'species' (three distinct types) and 'sex' (female and male), and the numerical measurements 'bill_length' and 'bill_depth'.
Our primary goal is to investigate the potential linear relationship between bill length and depth while assessing the influence of species and sex on these dimensions. This analysis will help us to identify any notable patterns.

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))

# Scatter plot of bill length vs bill depth for each species
ggplot(penguins, aes(x = bill_depth, y = bill_length, color = species)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Bill Length vs Bill Depth", x = "Bill Depth", y = "Bill Length")

# Scatter plot of bill length vs bill depth for each sex
ggplot(penguins, aes(x = bill_depth, y = bill_length, color = sex)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Bill Length vs Bill Depth", x = "Bill Depth", y = "Bill Length")
```
We can clearly see that the species of the penguins significantly influences the correlation between bill length and bill depth. This shows a distinct morphological variation across different species.  
However we can investigate a notable negative correlation overall. This trend is mainly driven by Adelie penguins, whose diet primarily consists of shellfish, unlike the fish-based diets of Chinstrap and Gentoo penguins. The dietary differences contribute to the distinct differences of bill morphology and must be taken into account.

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))

# Histogram of bill lengths for each species
ggplot(penguins, aes(x = bill_length, fill = species)) +
  geom_density(alpha = 0.7) +
  labs(title = "Density Distribution of Bill Lengths by Species", x = "Bill Length") +
  theme_minimal()

# Histogram of bill depth for each species
ggplot(penguins, aes(x = bill_depth, fill = species)) +
  geom_density(alpha = 0.7) +
  labs(title = "Density Distribution of Bill Depth by Species", x = "Bill Depth") +
  theme_minimal()
```

The histograms underline our assumption.

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))

# Histogram of bill lengths
ggplot(penguins, aes(x = bill_length, fill = "red")) +
  geom_density(alpha = 0.7) +
  labs(title = "Density Distribution of Bill Lengths", x = "Bill Length") +
  theme_minimal() +
  theme(legend.position = "none")

# Histogram of bill depth
ggplot(penguins, aes(x = bill_depth, fill = "blue")) +
  geom_density(alpha = 0.7) +
  labs(title = "Density Distribution of Bill Depth", x = "Bill Depth") +
  theme_minimal() +
  theme(legend.position = "none")
```

The distributions of both bill length and bill depth suggest that a single pooled model may not capture the complexities of the underlying data very well.   Their multimodel nature indicates potential issues, underlining further that distinct subgroups exist within the data, which a pooled approach could overlook.

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))

# Boxplot for bill length by species and sex
ggplot(penguins, aes(x = species, y = bill_length, fill = sex)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Bill Length by Species and Sex", x = "Species", y = "Bill Length (mm)")

# Boxplot for bill length by species and sex
ggplot(penguins, aes(x = species, y = bill_depth, fill = sex)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Bill Length by Species and Sex", x = "Species", y = "Bill Length (mm)")
```

The observed data reveal a distinct variation in bill depth and bill length between males and females within the same penguin species.  
This suggests that incorporating both species and sex as factors in hierarchical models could enhance the accuracy.

# 2. Modelling

In our analysis, we explore both non-hierarchical and hierarchical linear models to understand the relationship between bill length and depth in Antarctic penguins, while considering species and sex as influential factors.
We use a pooled non-hierarchical model which does not consider  species and sex as our baseline.

## Non-Hierarchical Model (Pooled)

```{r, include=FALSE}
# empirical Bayes prior
pooled_lm <- lm(penguins$bill_length~penguins$bill_depth)
alpha_empirical <- pooled_lm$coefficients[1]
beta_empirical <- pooled_lm$coefficients[2]
alpha_sd_empirical <- summary(pooled_lm)$coefficients[1,2]
beta_sd_empirical <- summary(pooled_lm)$coefficients[2,2]
```


```{r, include=FALSE}
data_nh_pool = list(J = length(penguins$bill_length),
                 X = penguins$bill_depth,
                 Y = penguins$bill_length
                 )
```


```{r, include=FALSE}
fit_pool_empirical = stan(file='Stan/pooled_empirical_bayes.stan', data=data_nh_pool)
```

In this model we assume that the Likelihood function of our response variable $Y$, representing bill length, follows a normal distribution. The mean of this distribution is defined as a linear function of the predictor X (bill depth).
$$Y \sim\mathcal{N}(\alpha + \beta \cdot X_i, \sigma)$$
where $X_i$ is the bill depth and $\alpha$ and $\beta$ are our normal prior distributions based on empirical estimates.  
We chose $\sigma\sim cauchy(0, 5)$ to allow for a wide range of possible values.

```{r, echo=FALSE, fig.show="hold", out.width="50%", message=FALSE}
par(mar = c(4, 4, .1, .1))
plot(fit_pool_empirical, par=c("alpha"))
plot(fit_pool_empirical, par=c("beta"))
```

We can observe that the estimated slope parameter ($\beta$) is negative, while the intercept is notably high. This suggests that in this simplified model, where individual variations across species and sex are not accounted for, there's an inverse relationship between bill depth and length.  
In our data observation we could already observe that this is not the case when we account for species.

## Non-Hierarchical Model (Separate)

```{r, include=FALSE}
# empirical Bayes prior
Adelie <-subset(penguins, penguins$species=="Adelie")
Chinstrap<-subset(penguins, penguins$species=="Chinstrap")
Gentoo <-subset(penguins, penguins$species=="Gentoo")

Adelie_sep_lm <- lm(data=Adelie,bill_length~bill_depth)
Chinstrap_sep_lm <- lm(data=Chinstrap,bill_length~bill_depth)
Gentoo_sep_lm <- lm(data=Gentoo,bill_length~bill_depth)

ad_alpha <- Adelie_sep_lm$coefficients[1]
ad_beta <- Adelie_sep_lm$coefficients[2]
gen_alpha <- Gentoo_sep_lm$coefficients[1]
gen_beta <- Gentoo_sep_lm$coefficients[2]
chin_alpha <- Chinstrap_sep_lm$coefficients[1]
chin_beta <- Chinstrap_sep_lm$coefficients[2]
```


```{r, include=FALSE}
penguins$group <- as.integer(as.factor(penguins$species))
data_sep <-list(N = length(penguins$bill_length),
                J = 3,
                group = penguins$group,
                X = penguins$bill_depth,
                Y = penguins$bill_length)
```


```{r, include=FALSE}
fit_sep_empirical = stan(file='Stan/sep_empirical_bayes.stan', data=data_sep)
```

In this model, we adopt a normal distribution for the response variable $Y$, representing bill length, with a group-specific mean. This mean is a linear function of the predictor X (bill depth), differentiated by group:
$$Y \sim\mathcal{N}(\alpha_{species} + \beta_{species} \cdot X_i, \sigma)$$
Here, $\alpha$[group[i]] and $\beta$[group[i]] represent the intercept and slope for each group, respectively, with each group corresponding to a different penguin species. These parameters are assigned normal priors based on empirical estimates.

```{r, echo=FALSE, fig.show="hold", out.width="50%", message=FALSE}
par(mar = c(4, 4, .1, .1))
plot(fit_sep_empirical, par=c("alpha"))
plot(fit_sep_empirical, par=c("beta"))
```

We can observe a significant difference in bill depth and length ratios among different penguin species, with Adelie penguins showing particularly distinct proportions.  
This finding highlights the importance of incorporating these differences into our modeling approach.

## Hierarchical Model (species + sex)

```{r, include=FALSE}
# empirical Bayes prior
stan_data_big <- list(
  N = nrow(penguins),
  S = length(unique(penguins$species_f)),
  G = length(unique(penguins$sex_f)),
  X = penguins$bill_depth,
  Y = penguins$bill_length,
  species_id = as.integer(penguins$species_f),
  sex_id = as.integer(penguins$sex_f)
)
```


```{r, include=FALSE}
fit_h_empirical_bayesian_estimate = stan(file='Stan/h_empirical_bayesian_estimate.stan', data=stan_data_big)
```

This model incorporates species and sex, allowing for varying intercepts and slopes. It is defined as:
$$Y\sim\mathcal{N}(\mu, \sigma_{obs})$$
$$\mu_i = (\alpha + \alpha_{species[i]} + \alpha_{sex[i]}) + (\beta + \beta_{species[i]} + \beta_{sex[i]}) \cdot X_i$$
with $\alpha$ and $\beta$ as the population-level prior for intercept and slope, $\alpha_{species[i]}$ and $\beta_{species[i]}$ as the adjustments priors specific to each species, $\alpha_{sex[i]}$ and $\beta_{sex[i]}$ as the adjustments priors specific to each gender and $\sigma_{obs}$, $\sigma_{species}$ and $\sigma_{sex}$ as the cauchy distributed standard deviations that capture the variability within species, sexes, and overall observations.

```{r,echo=FALSE,fig.show="hold", out.width="50%", message=FALSE}
plot(fit_h_empirical_bayesian_estimate, par=c("alpha_species"))
plot(fit_h_empirical_bayesian_estimate, par=c("beta_species"))
plot(fit_h_empirical_bayesian_estimate, par=c("alpha_sex"))
plot(fit_h_empirical_bayesian_estimate, par=c("beta_sex"))
```

The difference between the species-specific adjustments reflects our observations from before.  
In addition to that we can also observe difference between the genders which we also already mentioned in the data exploration section.

### Observation Models and Variable Selection Criteria
The observation model for both the non-hierarchical and hierarchical models is based on the assumption that bill length varies linearly with bill depth. This decision is supported by preliminary data analysis, which suggests a potential linear trend. The choice of species and sex as additional variables in the hierarchical model is based on biological reasoning and data observation that these factors can significantly impact penguin morphology.

### Choice of Priors
In our Bayesian analysis of the penguin dataset, we decided to utilize empirical Bayes priors, using classical linear regression (specifically the lm function in R) to set the means and standard deviation of the normally distributed priors for the parameters $\alpha$ and $\beta$ of all three of our Bayesian models.  
This method is especially useful when there's a lack of extensive external prior knowledge. This approach, while beneficial for stabilizing estimates and aiding computational efficiency, involves using the data twice—first to establish the priors, this means that one should be cautious against potential biases.  
For the parameter $\sigma$ we chose the Cauchy distribution, with its location parameter at 0 and scale parameter at 5, because of its heavier tails compared to a normal distribution, which allow for a wide range of possible values.
We used normal distributions centered at zero for the species and sex adjustment parameters $\alpha_{species[i]}$, $\beta_{species[i]}$, $\alpha_{sex[i]}$ and $\beta_{sex[i]}$. This choice reflects that deviations (both positive and negative) from the overall mean (alpha) are equally likely.


# 3. Model Checking

Model checking through posterior predictive checks involves evaluating the models' performance and their ability to accurately reflect the observed data. By simulating data from the posterior distributions and comparing these simulations to the actual data, we can assess how well each model captures the underlying structure and variability.


```{r, include=FALSE}
####Posterior predictive checks####
library(bayesplot)
library(ggplot2)
```

```{r, echo=FALSE, fig.show="hold", out.width="50%", include=FALSE}
# non-hierarchical pooled
par(mar = c(4, 4, .1, .1))
list_of_draws <- rstan::extract(fit_pool_empirical)
y_pred_pool_emp<- list_of_draws$y_new
#' Kernel density estimate of y + 100 yrep kernel density estimates
ppc_dens_overlay(penguins$bill_length, y_pred_pool_emp[1:500,])
#' ECDF of y + 500 yrep ECDFs
ppc_ecdf_overlay(penguins$bill_length, y_pred_pool_emp[1:500,])
#' Scatterplot of yrep vs y
ppc_scatter(penguins$bill_length, y_pred_pool_emp[1:4,])+geom_abline()
ppc_stat_2d(penguins$bill_length, y_pred_pool_emp, stat=c("min","max"))
```

```{r, echo=FALSE, fig.show="hold", out.width="50%", include=FALSE}
# non-hierarchical separate
par(mar = c(4, 4, .1, .1))
list_of_draws <- rstan::extract(fit_sep_empirical)
y_pred_sep_emp<- list_of_draws$y_new
#' Kernel density estimate of y + 100 yrep kernel density estimates
ppc_dens_overlay(penguins$bill_length, y_pred_sep_emp[1:500,])
#' ECDF of y + 500 yrep ECDFs
# ppc_ecdf_overlay(penguins$bill_length, y_pred_sep_emp[1:500,])
#' Scatterplot of yrep vs y
ppc_scatter(penguins$bill_length, y_pred_sep_emp[1:4,])+geom_abline()
# ppc_stat_2d(penguins$bill_length, y_pred_sep_emp, stat=c("min","max"))
```


```{r, echo=FALSE, fig.show="hold", out.width="50%", include=FALSE}
# hierarchical
par(mar = c(4, 4, .1, .1))
list_of_draws <- rstan::extract(fit_h_empirical_bayesian_estimate)
y_h_emp<- list_of_draws$y_new
#' Kernel density estimate of y + 100 yrep kernel density estimates
ppc_dens_overlay(penguins$bill_length, y_h_emp[1:500,])
#' ECDF of y + 500 yrep ECDFs
# ppc_ecdf_overlay(penguins$bill_length, y_h_emp[1:500,])
#' Scatterplot of yrep vs y
ppc_scatter(penguins$bill_length, y_h_emp[1:4,])+geom_abline()
# ppc_stat_2d(penguins$bill_length, y_h_emp, stat=c("min","max"))
```

```{r, echo=FALSE, fig.show="hold", out.width="33%"}
par(mar = c(4, 4, .1, .1))

# pooled model plot
list_of_draws_pool <- rstan::extract(fit_pool_empirical)
y_pool_emp <- list_of_draws_pool$y_new
ppc_dens_overlay(penguins$bill_length, y_pool_emp[1:500,])

# separate model plot
list_of_draws_sep <- rstan::extract(fit_sep_empirical)
y_sep_emp <- list_of_draws_sep$y_new
ppc_dens_overlay(penguins$bill_length, y_sep_emp[1:500,])

# hierarchical model plot
list_of_draws_h <- rstan::extract(fit_h_empirical_bayesian_estimate)
y_h_emp <- list_of_draws_h$y_new
ppc_dens_overlay(penguins$bill_length, y_h_emp[1:500,])
```

In this posterior predictive check (PPCs) using kernel density plots we show the differences in model performances. The pooled model shows a significant misalignment between the distributions, indicating a poor fit.  
In contrast, the separated model shows improvement, reflecting the importance of species-specific differences.  
However, the hierarchical model demonstrated the most precise fit, with a slightly smaller variance in predictions, suggesting that it reflects the real data the best.

```{r, echo=FALSE, fig.show="hold", out.width="33%"}
par(mar = c(4, 4, .1, .1))

# pooled model plot
list_of_draws_pool <- rstan::extract(fit_pool_empirical)
y_pool_emp <- list_of_draws_pool$y_new
ppc_scatter(penguins$bill_length, y_pool_emp[1:4,])+geom_abline()

# separate model plot
list_of_draws_sep <- rstan::extract(fit_sep_empirical)
y_sep_emp <- list_of_draws_sep$y_new
ppc_scatter(penguins$bill_length, y_sep_emp[1:4,])+geom_abline()

# hierarchical model plot
list_of_draws_h <- rstan::extract(fit_h_empirical_bayesian_estimate)
y_h_emp <- list_of_draws_h$y_new
ppc_scatter(penguins$bill_length, y_h_emp[1:4,])+geom_abline()
```

The Posterior Predictive Check Scatter Plot underlines our statement and highlights the higher variance of the non-hierarchical separate plot.

# 4. Model Comparison

## Comparison non-hierarchical non-informative with hierarchical non-informative (Leave-one-out validation)
In our model comparison we employed Leave-One-Out Cross-Validation (LOO-CV) as our method of choice for model comparison. LOO-CV provides a robust means of assessing each model's predictive performance, crucial for determining the most suitable approach for our data.  
LOO-CV calculates the Log Predictive Density (metric measures how likely the observed data point is under the model) for each observation.  
  
**elpd_diff:** model's ability to predict new data differs from the best model in your comparison. If elpd_diff is close to zero, the model predicts almost as well as the best one  
**se_diff:** standard error of the elpd_diff. It quantifies the uncertainty or variability in the elpd_diff estimate. A smaller se_diff suggests more confidence in the elpd_diff value, while a larger se_diff indicates more uncertainty.

```{r, include=FALSE}
library(loo)

loo_pooled_emp <- rstan::loo(fit_pool_empirical)
loo_separate_emp <- rstan::loo(fit_sep_empirical)
loo_hierarchical_emp <- rstan::loo(fit_h_empirical_bayesian_estimate)

comp <- loo_compare(list(loo_pooled_emp, loo_separate_emp, loo_hierarchical_emp))
comp

# print(comp,simplify = FALSE,digits=3)
```




```{r,echo=FALSE, , message=FALSE, error=FALSE}
library(dplyr)
comp_table <- as.data.frame(comp)
# Create a table with model name, elpd_diff, and se_diff
table_data <- data.frame(
  model=c("Hierarchical", "Separate", "Pooled"),
  ELPD_Diff = comp_table$elpd_diff,
  SE_Diff = comp_table$se_diff
)

kable(table_data, "markdown")

```


The LOO-CV results clearly show that the hierarchical model (model3) outperforms the others in predicting new data, with the separated model (model2) and the pooled model (model1) trailing by elpd_diff values of -28.1 and -286.5, respectively. This significant difference in predictive accuracy, especially between the hierarchical and pooled models, highlights the importance of considering species and sex-specific variations in the analysis.


```{r, echo=FALSE}
# Create a data frame for plotting
loo_data <- data.frame(
  model = c("Model 3", "Model 2", "Model 1"),
  elpd_diff = unname(comp[,1]),
  se_diff = unname(comp[,2])
)

# Error bar plot
ggplot(loo_data, aes(x = model, y = elpd_diff)) +
  geom_point() +
  geom_errorbar(aes(ymin = elpd_diff - se_diff, ymax = elpd_diff + se_diff), width = 0.2, color="blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Model Predictive Performance (LOO-CV)",
       x = "Model",
       y = "ELPD Difference") +
  theme_minimal()
```

This plot highlights the big difference between Model2/Model3 to Model1.

# 5. Sensitivity Analysis

When conducting Sensitivity Analysis, our objective is to assess the impact of the chosen prior on our model. This enables us to evaluate the robustness of our model by testing it with various priors and comparing their performances.

## Pooled Model

Let's begin by examining our pooled model. Identifying informative prior information beyond our dataset proves challenging. Scant data is available on Adelie, Chinstrap, and Gentoo penguins outside of our dataset. An initial notion involved utilizing bill length and bill depth from a broader penguin species range as prior information. However, given the interrelated nature of these three penguin species, this idea inherently constitutes an ill-suited prior.

An alternative approach to acquiring prior information for our penguin data involves consulting ChatGPT about the bill lengths of our three penguin species. Information received indicates a range of 40 to 45 millimeters for Adelie and Chinstrap penguins, and 47 to 48 millimeters for Gentoo penguins. While the data source remains unclear, these estimates appear reasonable, particularly for Adelie and Gentoo penguins.

Further, we can evaluate our model with a weakly informative prior, employing a broad normal distribution for all parameters. To assess sensitivity, we deliberately test the model with suboptimal prior information, $\alpha \sim \mathcal{N}(0, 5)$, recognizing that this value is notably low.

Given the challenge of finding a truly informative prior, our most confident approach involves empirical calculation. This entails fitting a linear model and utilizing the distribution of its intercept and slope as prior information. Additionally, we incorporate the mean and standard deviation of our data into the prior distribution. This approach not only addresses the difficulty of obtaining informative priors but also leverages the inherent patterns within our dataset for a more nuanced analysis.


```{r, include=FALSE}
fit_pool_sd_mean = stan(file = 'Stan/pooled_sd_mean.stan', data = data_nh_pool)
fit_pool_non_inf = stan(file = 'Stan/pooled_non_inf.stan', data = data_nh_pool)
fit_pool_chatgpt = stan(file = 'Stan/pooled_chatgpt.stan', data = data_nh_pool)
fit_pool_poor_inf = stan(file = 'Stan/pooled_poor_inf.stan', data = data_nh_pool)
```

```{r, include=FALSE}
library(loo)

loo_pool_empirical <- rstan::loo(fit_pool_empirical)
loo_pool_sd_mean <- rstan::loo(fit_pool_sd_mean)
loo_pool_non_inf <- rstan::loo(fit_pool_non_inf)
loo_pool_chatgpt <- rstan::loo(fit_pool_chatgpt)
loo_pool_poor_inf <- rstan::loo(fit_pool_poor_inf)
```


Let us compare the pooled model for different priors.
```{r,include=FALSE}
comp <- loo_compare(list(loo_pool_empirical, loo_pool_sd_mean, loo_pool_non_inf, loo_pool_chatgpt, loo_pool_poor_inf))
comp
```

```{r,echo=FALSE, , message=FALSE, error=FALSE}
library(dplyr)
comp_table <- as.data.frame(comp)
# Create a table with model name, elpd_diff, and se_diff
table_data <- data.frame(
  model=c("Empirical", "Weakly Informative", "SD/Mean", "ChatGPT", "Poor Informative"),
  ELPD_Diff = comp_table$elpd_diff,
  SE_Diff = comp_table$se_diff
)

kable(table_data, "markdown")

```


Convergence with Empirical Prior:

```{r, echo=FALSE, fig.dim=c(8,3)}
rstan::traceplot(fit_pool_empirical,pars=c("alpha","beta"), inc_warmup=TRUE, nrow=3)
```

Convergence with poor Prior:

```{r, echo=FALSE, fig.dim=c(8,3)}
rstan::traceplot(fit_pool_poor_inf,pars=c("alpha","beta"), inc_warmup=TRUE, nrow=3)
```
The Empirical prior performs better, but the poorly chosen one does not look horrendous.
As expected, the empirical prior performs exceptionally well in Leave-One-Out Cross-Validation. Most models demonstrate commendable performance, except for the one deliberately tested with a suboptimal prior. This divergence emphasizes the model's sensitivity to the influence of a poorly chosen prior.

## Separate Model
Shifting our focus to the separate model, we explore different priors, this time dividing our data into three distinct groups corresponding to the three penguin species in our dataset. Each species is assigned its own prior distribution, with the exception of the minimally informative and intentionally flawed priors. Additionally, we assess the empirical prior derived from the pooled model.

This approach not only offers a nuanced insight into the model's performance across various species but also underscores the importance of tailoring priors to specific subsets of data. Deliberately considering species-specific priors acknowledges the potential underlying variations among the species, prompting the need for distinct prior information to achieve optimal model calibration.

```{r, include=FALSE}
penguins$group <- as.integer(as.factor(penguins$species))
data_sep <-list(N = length(penguins$bill_length),
                J = 3,
                group = penguins$group,
                X = penguins$bill_depth,
                Y = penguins$bill_length)
```

```{r, include=FALSE}
fit_sep_empirical = stan(file='Stan/sep_empirical_bayes.stan', data=data_sep)
fit_sep_empirical2 = stan(file = 'Stan/sep_empirical_bayes2.stan', data=data_sep)
fit_sep_sd_mean = stan(file = 'Stan/sep_sd_mean2.stan', data = data_sep)
fit_sep_non_inf = stan(file = 'Stan/sep_non_inf.stan', data = data_sep)
fit_sep_chatgpt = stan(file = 'Stan/sep_chatgpt.stan', data = data_sep)
```


```{r, include=FALSE}
fit_sep_poor_inf = stan(file = 'Stan/sep_poor_inf.stan', data = data_sep)
```


```{r, include=FALSE}
library(loo)

loo_sep_empirical <- rstan::loo(fit_sep_empirical)
loo_sep_empirical2 <- rstan::loo(fit_sep_empirical2)
loo_sep_sd_mean <- rstan::loo(fit_sep_sd_mean)
loo_sep_non_inf <- rstan::loo(fit_sep_non_inf)
loo_sep_chatgpt <- rstan::loo(fit_sep_chatgpt)
loo_sep_poor_inf<- rstan::loo(fit_sep_poor_inf)
```

```{r,include=FALSE}
comp <- loo_compare(list(loo_sep_empirical,loo_sep_empirical2, loo_sep_sd_mean, loo_sep_non_inf, loo_sep_chatgpt, loo_sep_poor_inf))
comp
```


```{r,echo=FALSE, , message=FALSE, error=FALSE}
library(dplyr)
comp_table <- as.data.frame(comp)
# Create a table with model name, elpd_diff, and se_diff
table_data <- data.frame(
  model=c("Empirical", "Weakly Informative", "Poor Informative", "SD/Mean", "ChatGPT", "Pooled Empirical"),
  ELPD_Diff = comp_table$elpd_diff,
  SE_Diff = comp_table$se_diff
)

kable(table_data, "markdown")
```

Convergence with Empirical Prior:

```{r, echo=FALSE, fig.dim=c(8,3)}
rstan::traceplot(fit_sep_empirical,pars=c("alpha","beta"), inc_warmup=TRUE, nrow=3)
```

Convergence with poor Prior:

```{r, echo=FALSE, fig.dim=c(8,3)}
rstan::traceplot(fit_sep_poor_inf,pars=c("alpha","beta"), inc_warmup=TRUE, nrow=3)
```

Again, both Priors perform reasonably well, but the empirical one still looking supreme.

Once more, the empirical prior emerges as the top performer in Cross-Validation. Surprisingly, the pooled empirical prior shows the weakest performance, with the mean/standard deviation trailing behind our intentionally suboptimal prior. Nevertheless, the weakly informative prior continues to yield reasonably sound results.


## Hierarchical Model
Moving on to the hierarchical model, the pursuit of the empirical estimate takes on added complexity. Unlike the simplicity of obtaining empirical estimates in our earlier models through linear fitting, the presence of a hyperparameter necessitates an additional step. One idea is to recursively determine the distribution of the hyperparameter.
Determining the Bayesian Empirical Estimate requires calculating the predictive posterior distribution.

Using the Loss function $L(\hat{\theta}-\theta)²\ge0$ to minimize the loss of our priors gives us a the mean of the predicted posterior distribution as Bayesian Point Estimation, which we will use as our empirical prior. The empirical variance is then basically the variance from all our estimated means.

$X \sim N(\mu, \sigma^2), p(\mu, \sigma^2)\propto \frac{1}{\sigma^2}$

$f(x_i | \mu, \sigma^2) = \frac{1}{2\pi} \sigma \exp\left(\frac{1}{2\sigma^2}\left(X_i - \bar{X}\right)^2\right)$

$L(\mu, \sigma^2 | \bar{X}) = (2\pi)^\frac{-n}{2} \cdot (\sigma^2)^\frac{-n}{2} \cdot \exp\left(\frac{-1}{2\sigma^2} \sum_{i=1}^{n} (X_i - \bar{X})^2\right)$

$p(\mu,\sigma^2 | \bar{X}) \propto p(\mu, \sigma^2) \cdot L(\mu, \sigma^2 | \bar{X})$

$p(\sigma^2 | \bar{X}) = \int_{-\infty}^{\infty} p(\mu, \sigma^2 | \bar{X}) \, d\mu$

$p(\sigma^2 | \bar{X}) \propto \text{InvGamma}\left(\frac{n-1}{2}, \frac{2}{(n-1)\sigma^2}\right)$

$\text{mean} = P(\theta^2 | \bar{X}) = \frac{1}{\beta(\alpha-1)}$

$\sigma^2 = \frac{1}{n-3}\sum_{i=1}^{n} (X_i - \bar{X})^2$



Initiating this process requires a prior distribution. Given our lack of confidence in identifying an informative prior, we resort to employing an uninformative one. Subsequently, we calculate the predicted posterior distribution, yielding a valuable empirical estimate. This intricate approach ensures a robust estimation of the hyperparameter, contributing to a more nuanced and accurate portrayal of the hierarchical model's dynamics.

```{r, include=FALSE}
fit_posterior<- stan(file = 'h_posterior.stan', data=stan_data_big)
```

```{r, echo=FALSE, , message=FALSE, error=FALSE}
summary(fit_posterior, par=(c("alpha", "beta", "alpha_species", "beta_species", "alpha_sex",
                                   "beta_sex", "sigma_species", "sigma_sex", "sigma_obs")))$summary[,c(1,3,9,10)]
```

All the values for $\hat{R}$ are very close to one, which is good.

```{r,echo=FALSE, , message=FALSE, error=FALSE, warning=FALSE}
library(bayesplot)
library(ggplot2)
library(gridExtra)
library(cowplot)

posterior_samples_flat <- as.array(fit_posterior)

params_to_plot <- c("alpha", "beta", "sigma_species", "sigma_sex", "sigma_obs")

# Create a list of plots
plots_list <- lapply(params_to_plot, function(param) {
  ggplot(mapping = aes(x = posterior_samples_flat[, , param])) +
    geom_histogram(fill = "skyblue", color = "black") +
    geom_vline(aes(xintercept = mean(posterior_samples_flat[, , param])),
               color = "red", linetype = "dashed", size = 1) +
    labs(x = param, y = NULL)
})
# Arrange and display the plots
grid.arrange(grobs = plots_list, ncol = 3)

```


```{r, include=FALSE}
posterior_samples <- as.array(fit_posterior)
mcmc_hist(posterior_samples[, , "alpha"], binwidth = 0.1)

mcmc_hist(posterior_samples[, , "beta"], binwidth = 0.01)

mcmc_hist(posterior_samples[, , "sigma_species"], binwidth = 0.05)

mcmc_hist(posterior_samples[, , "sigma_sex"], binwidth = 0.05)

mcmc_hist(posterior_samples[, , "sigma_obs"], binwidth = 0.01)
```


Our examination indicates that a normal distribution seems to be a good fit for most of our data. However, there is lingering uncertainty when it comes to the standard deviation for our factors, specifically sex and species. This uncertainty arises because the distribution not only resembles a positive normal but also displays characteristics similar to a Gamma or, possibly, a Lognormal function.

Given this uncertainty, we use the information obtained from the predicted posterior distribution as our Empirical Bayesian Estimator. To address the uncertainty around the choice of distributions, we take a cautious approach by fitting the model three times: once with a normal distribution, once with a Gamma distribution, and once with a Lognormal distribution.

We determine our $\alpha,\beta$ and meanlog/sdlog parameters using the 'fitdist' function from the 'MASS' package. This function approximates our parameters.


```{r, include=FALSE}
library(MASS)
psigma <- posterior_samples[, , "sigma_species"]
p <-c(psigma[,1],psigma[,2],psigma[,3],psigma[,4])
# Fit a gamma distribution to your data
fit <- fitdistr(p, densfun = "gamma")

# Alternatively, you can use gamma.fit
# fit <- gamma.fit(alpha_samples)

# Display the fitted parameters
shape <- fit$estimate["shape"]
rate <- fit$estimate["rate"]

# Alternatively, if using gamma.fit
# shape <- fit$alpha
# rate <- fit$beta
```


```{r, include=FALSE}
# Print the fitted parameters
cat("Shape:", shape, "\n")
cat("Rate:", rate, "\n")
```


```{r, include=FALSE}
# Fit a log-normal distribution to your data
fit <- fitdistr(p, densfun = "lognormal")

# Display the fitted parameters
log_mean <- fit$estimate["meanlog"]
log_sd <- fit$estimate["sdlog"]
```


```{r, include=FALSE}
# Print the fitted parameters
cat("Log-Mean:", log_mean, "\n")
cat("Log-SD:", log_sd, "\n")
```


```{r, include=FALSE}
psigma <- posterior_samples[, , "sigma_sex"]
p <-c(psigma[,1],psigma[,2],psigma[,3],psigma[,4])
# Fit a gamma distribution to your data
fit <- fitdistr(p, densfun = "gamma")

# Alternatively, you can use gamma.fit
# fit <- gamma.fit(alpha_samples)

# Display the fitted parameters
shape <- fit$estimate["shape"]
rate <- fit$estimate["rate"]

# Alternatively, if using gamma.fit
# shape <- fit$alpha
# rate <- fit$beta
```


```{r, include=FALSE}
# Print the fitted parameters
cat("Shape:", shape, "\n")
cat("Rate:", rate, "\n")
```


```{r, include=FALSE}
# Fit a log-normal distribution to your data
fit <- fitdistr(p, densfun = "lognormal")

# Display the fitted parameters
log_mean <- fit$estimate["meanlog"]
log_sd <- fit$estimate["sdlog"]
```


```{r, include=FALSE}
# Print the fitted parameters
cat("Log-Mean:", log_mean, "\n")
cat("Log-SD:", log_sd, "\n")
```

```{r, include=FALSE}
fit_posterior_weak <- stan(file = 'Stan/h_posterior_weak.stan', data=stan_data_big)
fit_empirical_bayesian_estimate<- stan(file='Stan/h_empirical_bayesian_estimate.stan', data=stan_data_big)
fit_gamma <- stan(file='Stan/weak_gamma.stan', data=stan_data_big)
fit_lognormal<-stan(file = 'Stan/weak_lognormal.stan', data=stan_data_big)
fit_non_inf<-stan(file = 'Stan/h_non_inf.stan', data=stan_data_big)

fit_h_poor<-stan(file = 'Stan/h_poor.stan', data=stan_data_big)
```


```{r, include=FALSE}
fit_inv_gamma<-stan(file='Stan/h_inv_gamma.stan', data=stan_data_big)
```


```{r, include=FALSE}
library(loo)
loo_bayesian_estimate<-rstan::loo(fit_empirical_bayesian_estimate)
loo_gamma<-rstan::loo(fit_gamma)
loo_inv_gamma<-rstan::loo(fit_inv_gamma)
loo_lognormal<-rstan::loo(fit_lognormal)
loo_posterior_pred<-rstan::loo(fit_posterior)
loo_non_inf<-rstan::loo(fit_non_inf)
loo_h_poor<-rstan::loo(fit_h_poor)
```

Convergence with Empirical Prior:

```{r, echo=FALSE, fig.dim=c(8,3)}
rstan::traceplot(fit_empirical_bayesian_estimate,pars=c("alpha","beta","sigma_sex","sigma_species","sigma_obs"), inc_warmup=TRUE, nrow=3)
```

Convergence with poor Prior:

```{r, echo=FALSE, fig.dim=c(8,3)}
rstan::traceplot(fit_h_poor,pars=c("alpha","beta","sigma_sex","sigma_species","sigma_obs"), inc_warmup=TRUE, nrow=3)
```

In all cases the poor prior converges much worse, showing prior sensitivity for convergence.

```{r,include=FALSE}
comp<-loo_compare(loo_bayesian_estimate,loo_gamma,loo_lognormal,loo_posterior_pred,loo_non_inf, loo_inv_gamma, loo_h_poor)
comp
```

```{r,echo=FALSE, , message=FALSE, error=FALSE}
library(dplyr)
comp_table <- as.data.frame(comp)
# Create a table with model name, elpd_diff, and se_diff
table_data <- data.frame(
  model=c("Empirical", "Gamma", "Posterior", "Lognormal","Inverse Gamma", "Weakly informative", "Poor"),
  ELPD_Diff = comp_table$elpd_diff,
  SE_Diff = comp_table$se_diff
)

kable(table_data, "markdown")



```


Our comparative analysis shows that the Empirical Bayesian Estimates obtained with the Normal Distribution and the Gamma Distribution stand out as the top performers. The Lognormal Distribution performs quite a but worse. Notably, the weakly informative prior exhibits the least favorable performance among the approaches we evaluated.

This outcome emphasizes the critical role of distribution selection in achieving optimal results. The robust performance of the Empirical Bayesian Estimates derived from the Normal and Gamma Distributions suggests that these distributions align more closely with the underlying dynamics of our data.

# 6. Discussion:

### Pooled Model Challenges:

The use of a single pooled model for all penguin species may oversimplify the underlying complexities. The assumption that all species share a common linear relationship between bill length and depth might not accurately capture the nuanced variations present in different species.

### Prior Information Challenges:

Acquiring informative prior information for species-specific characteristics remains a challenge. While attempting to incorporate ChatGPT suggestions, the lack of a clear data source raises questions about the reliability of the prior estimates, especially for Adelie and Gentoo penguins.

### Species and Sex Influence:

The distinct impact of species and sex on penguin morphology becomes evident. However, the data exploration hints at potential interactions between these factors, suggesting a need for more sophisticated models that account for these nuances.

### Model Robustness:

The sensitivity analysis reveals the robustness of the empirical prior, but the poorly chosen priors, especially in the pooled model, demonstrate the model's susceptibility to prior selection. A more comprehensive exploration of informative priors is essential for a well-rounded Bayesian analysis.

### Model Comparison:

While the hierarchical model outperforms others in predictive accuracy, the considerable difference in performance between models raises questions about the adequacy of the simpler models. Further exploration of model structures and additional factors might enhance predictive capabilities.


# 7. Conclusion

In conclusion, our analysis of the penguin dataset provided valuable insights into modeling and estimation. We explored three different models and tested different priors to understand their impact on model performance. Remarkably, the empirical Bayesian approach, using information from our dataset, consistently proved to be a strong choice, performing better than other priors, given that it was challenge to come up with actual informative prior information, since there is a lack of information about our penguin species outside of our specific dataset.

Our examination of the hierarchical model added additional complexities, requiring the estimation of hyperparameters. Despite uncertainties in choosing informative priors, our careful approach, including uninformative priors and empirical estimates, provided a detailed understanding of the model dynamics.

In most cases the normal distribution seemed to be the best fit, we only used some Cauchy distributions for some standard deviations and in our hierarchical model some Gamma distributions. We also tried a bit more bold attempt with a Lognormal distribution, which did not perform as well.
The comparison of distribution fits highlighted the importance of choosing distributions that closely match the underlying data characteristics.

In essence, our findings emphasize the importance of thoughtful model selection, prior specification, and distribution choices in Bayesian estimation. The empirical Bayesian approach, rooted in our dataset, emerged as a reliable strategy, demonstrating its effectiveness in improving model accuracy and reliability.

# 8. AI Disclosure
In this project, we asked about informative prior information from ChatGPT, recognizing the potential challenge of its performance.
Additionally, ChatGPT played a helpful role in formulating our points and assisted in writing code for graph plotting.

# 9. Appendix

Our Code can be foung on Github:

https://github.com/floom9/Project-Penguins.git

